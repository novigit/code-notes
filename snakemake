#!/bin/bash

# snakemake is a workflow management system (WMS)

# it's based on a 'pull' kind of system
## you ask for output files and snakemake will use your Snakefile to figure out how to generate the requested output files (pull)
## with a bash pipeline you provide the input files and bash will use the bash script to generate the output files (push)

SNAKEFILE - TIPS AND TRICKS

# expand()
expand('{sample}.{direction}.fastqc.html', sample = ['sample1','sample2'], direction = ['fw', 'rv'])
## will generate
## sample1.fw.fastqc.html
## sample1.rv.fastqc.html
## sample2.fw.fastqc.html
## sample2.rv.fastqc.html
# useful to compactly state your desired output files in the all rule


# input functions
def get_fastq(wildcards):
    if wildcards.sample == 'sample1':
        return 'sample1/sample1.fw.fastq.gz'
    elif wildcards.sample == 'sample2':
        return 'sample2/sample2.fw.fastq.gz'

rule fastqc:
    input: get_fastq
## useful to process outputs from different parts of the pipeline in a single rule

def get_pilon_input(wildcards):
    input_dict = {
        'assembly' : '{wildcards.assembly}.fasta'.format(wildcards = wildcards),
        'bam'      : '{wildcards.assembly}.bam'.format(wildcards = wildcards)
    }
    return input_dict

rule pilon:
    input: unpack(get_pilon_input)
    shell: 'pilon --genome {input.assembly} --bam {input.bam}'
## useful if you have multiple input files that vary depending on the requested output

## input functions need to be stated ABOVE the rules that call them in the Snakefile


# using a lambda expression
rule repeatmasker:
    threads: 8
    params:
        jobs=lambda wildcards, threads : threads // 4
    shell: 'RepeatModeler -p {params.job}'
## useful if there is a maximum amount of threads you can use per 'job'
## // means floor division. E.g., 9 // 4 = 2


# define which rules should not be submitted to the cluster
# i.e., execute these on the headnode
localrules: all, rule_a, rule_b

SNAKEMAKE COMMAND LINE - TIPS AND TRICKS

# run a snakefile on an HPC cluster
snakemake --cluster 'qsub -V'
snakemake --cluster 'sbatch'
# each 'snakejob' will be submitted seperately with qsub / sbatch
## when on a Sun Grid Engine cluster, make sure you are in
## the environment that has all required tools loaded
## before executing the snakefile. 
## -V ensures that your environment variables are passed on to the submitted job

# submit each snakejob with the number of threads specified in the rule
snakemake --cluster 'qsub -V -pe threaded {threads}'

# limit the number of jobs that can be submitted to the HPC queue at the same time
snakemake --cluster 'qsub -V' --jobs 6

# specify the location of the STDOUT and STDERR files of each submitted snakejob
snakemake --cluster 'qsub -V -o logs/{rule}.{jobid}.o -e logs/{rule}.{jobid}.e'
## where {rule} and {jobid} are special wildcards holding the rule name and jobid of the snakejob, respectively
## if not specified, the STDOUT and STDERR files are stored in your $HOME

# only trigger a rerun using modification times
snakemake --rerun-triggers mtime
## default is mtime,params,input,software_env,code
## which is very aggressive

# mark certain output files as up-to-date, so
# that their associated rule will not rerun even if their input files are newer
snakemake --touch outputfile

SNAKEMAKE GENERAL EXECUTION NOTES

- After a rule is executed, SNAKEMAKE will check if all your desired output files stated in 'output:' are actually present in your filesystem.
If at least one of them is not present, SNAKEMAKE will assume the rule executation failed and remove all files generated by that rule / job

